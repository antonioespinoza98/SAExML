---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Modelo con solo covariables geo espaciales

```{r}
rm(list = ls()) 
```

## Datos

```{r}
encuesta_mrp <- readRDS("ingreso/datos/encuesta_mrp1.rds")
```

```{r}
# Encuesta quitamos ingreso, lp, li y fep
encuesta_mrp <- encuesta_mrp %>% 
  select(
    F182013_stable_lights, X2016_crops.coverfraction, X2016_urban.coverfraction, X2016_gHM, accessibility, 
    accessibility_walking_only, ingreso
  )

summary(encuesta_mrp)
```

```{r}
# Debido a que el XGBoosting solo toma valores numericos, se debe hacer una pequeña transformación para hacerlo funcionar.

sparse_matrix <- sparse.model.matrix(ingreso ~ ., data = encuesta_mrp)[, -1]

y = encuesta_mrp[,7]

encuesta_mrp_m <- xgb.DMatrix(data = sparse_matrix,
                              label = as.matrix(sapply(y, as.numeric)))
```

```{r, eval=FALSE}
grid <- expand_grid(
  max_depth = seq(6, 10, 1),
  lambda = seq(0, 5, 1),
  lambda_bias = seq(0, 5, 1),
  alpha = seq(0, 5, 1)
)
```


## Validación cruzada

```{r, eval=FALSE}

xgb_train_rmse <- numeric(nrow(grid))
xgb_test_rmse <- numeric(nrow(grid))


for(i in 1:nrow(grid)){
  xgb_untuned = xgb.cv(
    data = encuesta_mrp_m,
    booster = "gblinear",
    lambda = grid$lambda[i],
    alpha = grid$alpha[i],
    max_depth = grid$max_depth[i],
    lambda_bias = grid$lambda_bias[i],
    objective = "reg:squarederror",
    eval_metric = "rmse",
    nrounds = 1000,
    early_stopping_rounds = 3, # training with a validation set will stop if the performance does not improve for k rounds (3)
    nfold = 5
  )
  
  xgb_train_rmse[i] <-
    xgb_untuned$evaluation_log$train_rmse_mean[xgb_untuned$best_iteration]
  xgb_test_rmse[i] <-
    xgb_untuned$evaluation_log$test_rmse_mean[xgb_untuned$best_iteration]
  
  cat(i, "\n")
}
```

```{r}
xgb_train_rmse <- xgb_train_rmse %>% 
  tibble() %>% 
  mutate(simulacion = seq(1:1080))

xgb_test_rmse <- xgb_test_rmse %>% 
  tibble() %>% 
  mutate(simulacion = seq(1:1080))

xgb_train_rmse %>% ggplot(aes(x = simulacion, y = .)) + 
  geom_line()
```

```{r}
#Guardamos las iteraciones para el primer modelo de la validación cruzada

# saveRDS(xgb_train_rmse, file = "ingreso/output/xgb_train_rmse_geo.rds")
# saveRDS(xgb_test_rmse, file = "ingreso/output/xgb_test_rmse_geo.rds")
```


```{r}
xgb_train_rmse <- readRDS("ingreso/output/xgb_train_rmse_geo.rds")

grid[which.min(xgb_train_rmse$.), ] 

xgb_train_rmse %>% ggplot(aes(x = simulacion, y = .)) + 
  geom_line() + 
  geom_vline(xintercept = 270, linetype = 2, col = "red") + theme_minimal()
```

max depth: 7
lambda: 1
lambda bias: 2
alpha: 5

```{r}
modelo1 <- xgboost(
  booster = "gblinear",
  objective = "reg:squarederror",
  max_depth = 7,
  lambda = 1,
  lambda_bias = 2,
  alpha = 5,
  nrounds = 1000,
  early_stopping_rounds = 3,
  data = encuesta_mrp_m
)

ggplot(modelo1$evaluation_log, aes(
  x = iter,
  y = train_rmse
)) + geom_line() 

# Valor minimo
modelo1$evaluation_log[which.min(modelo1$evaluation_log$train_rmse), ]

eval <- modelo1$evaluation_log

ggplot(modelo1$evaluation_log, aes(
  x = iter,
  y = train_rmse
)) + geom_line() +
  geom_vline(xintercept = 374, col = "red", linetype = 2) + theme_minimal()
```

```{r}
saveRDS(modelo1, file = "ingreso/output/modelo1.rds")
```


## importancia de variables

```{r}
importancia <- xgb.importance(
  model = modelo1
)

p2 <- xgb.ggplot.importance(importancia)
```

# Modelo con solo las variables censales

## Datos

```{r}
rm(list = ls())
```

```{r, eval=FALSE}
encuesta_mrp <- data.table(readRDS("ingreso/datos/encuesta_mrp1.rds"))
```


```{r, eval=FALSE}
# Encuesta quitamos ingreso, lp, li y fep
encuesta_mrp <- encuesta_mrp %>% 
  select(
    area, ingreso,sexo, anoest, edad, discapacidad, 
    tiene_alcantarillado, tiene_electricidad, tiene_acueducto,
    tiene_gas, eliminar_basura, tiene_internet, piso_tierra,
    material_paredes, material_techo, rezago_escolar, alfabeta,
    hacinamiento, tasa_desocupacion
  ) %>% 
  mutate_if(is.character, as.factor) 

summary(encuesta_mrp)
```

```{r}
# Debido a que el XGBoosting solo toma valores numericos, se debe hacer una pequeña transformación para hacerlo funcionar.

sparse_matrix <- sparse.model.matrix(ingreso ~ ., data = encuesta_mrp)[, -1]
y = encuesta_mrp[,2]

encuesta_mrp_m <- xgb.DMatrix(data = sparse_matrix,
                              label = as.matrix(sapply(y, as.numeric)))
```


## Validación cruzada

```{r, eval=FALSE}
grid <- expand_grid(
  max_depth = seq(6, 10, 1),
  lambda = seq(0, 5, 1),
  lambda_bias = seq(0, 5, 1),
  alpha = seq(0, 5, 1)
)
```



```{r, eval=FALSE}
xgb_train_rmse <- numeric(nrow(grid))
xgb_test_rmse <- numeric(nrow(grid))


for(i in 1:nrow(grid)){
  xgb_untuned = xgb.cv(
    data = encuesta_mrp_m,
    booster = "gblinear",
    lambda = grid$lambda[i],
    alpha = grid$alpha[i],
    max_depth = grid$max_depth[i],
    lambda_bias = grid$lambda_bias[i],
    objective = "reg:squarederror",
    eval_metric = "rmse",
    nrounds = 1000,
    early_stopping_rounds = 3, # training with a validation set will stop if the performance does not improve for k rounds (3)
    nfold = 5
  )
  
  xgb_train_rmse[i] <-
    xgb_untuned$evaluation_log$train_rmse_mean[xgb_untuned$best_iteration]
  xgb_test_rmse[i] <-
    xgb_untuned$evaluation_log$test_rmse_mean[xgb_untuned$best_iteration]
  
  cat(i, "\n")
}
```

```{r, eval=FALSE}
xgb_train_rmse <- xgb_train_rmse %>% 
  tibble() %>% 
  mutate(simulacion = seq(1:1080))

xgb_test_rmse <- xgb_test_rmse %>% 
  tibble() %>% 
  mutate(simulacion = seq(1:1080))

xgb_train_rmse %>% ggplot(aes(x = simulacion, y = .)) + 
  geom_line()
```

```{r}
#Guardamos las iteraciones para el primer modelo de la validación cruzada

saveRDS(xgb_train_rmse, file = "ingreso/output/xgb_train_rmse_census.rds")
saveRDS(xgb_test_rmse, file = "ingreso/output/xgb_test_rmse_census.rds")
```

```{r}
xgb_train_rmse <- readRDS("ingreso/output/xgb_train_rmse_geo.rds")

grid[which.min(xgb_train_rmse$.), ] 


xgb_train_rmse %>% ggplot(aes(x = simulacion, y = .)) + 
  geom_line() + 
  geom_vline(xintercept = 1, linetype = 2, col = "red") +
  xlim(1,100) +
 theme_minimal()
```

 - max depth = 6
 - lambda = 0
 - lambda_bias = 0
 - alpha = 0

```{r, eval=FALSE}
modelo2 <- xgboost(
  booster = "gblinear",
  objective = "reg:squarederror",
  max_depth = 6,
  lambda = 0,
  lambda_bias = 0,
  alpha = 0,
  nrounds = 1000,
  early_stopping_rounds = 3,
  data = encuesta_mrp_m
)
```

```{r}
saveRDS(modelo2, file = "ingreso/output/modelo2.rds")
```

```{r}
ggplot(modelo2$evaluation_log, aes(
  x = iter,
  y = train_rmse
)) + geom_line() 

# Valor minimo
modelo2$evaluation_log[which.min(modelo2$evaluation_log$train_rmse), ]

eval <- modelo2$evaluation_log

ggplot(modelo2$evaluation_log, aes(
  x = iter,
  y = train_rmse
)) + geom_line() +
  geom_vline(xintercept = 121, col = "red", linetype = 2) + theme_minimal()

```

-   Para este caso podemos observar que luego de 927 iteraciones el modelo se detuvo debido a que el modelo no mejoró luego de 927 por lo que ahí se detuvo.
-   Por otro lado, se puede observar que luego de 104 iteraciones el modelo estabiliza el Error Cuadrático Medio.

## importancia de variables

```{r, eval=FALSE}
importancia <- xgb.importance(
  model = modelo2
)

p3 <- xgb.ggplot.importance(importancia)
```


# Modelo ajustado con parámetros de regularización mas pequeños


```{r}
# Encuesta quitamos ingreso, lp, li y fep
encuesta_mrp <- data.table(readRDS("ingreso/datos/encuesta_mrp1.rds")) %>% 
  select(-dam,-lp,-li,-fep) %>% 
  mutate_if(is.character, as.factor)

summary(encuesta_mrp)
head(encuesta_mrp)
str(encuesta_mrp)
```

```{r, eval=FALSE}
grid <- expand_grid(
  max_depth = seq(6, 10, 1),
  lambda = seq(0, 1, 0.15),
  lambda_bias = seq(0, 1, 0.15),
  alpha = seq(0, 1, 0.15)
)
```

```{r}
# Debido a que el XGBoosting solo toma valores numericos, se debe hacer una pequeña transformación para hacerlo funcionar.

sparse_matrix <- sparse.model.matrix(ingreso ~ ., data = encuesta_mrp)[, -1]
y = encuesta_mrp[,2]

encuesta_mrp_m <- xgb.DMatrix(data = sparse_matrix,
                              label = as.matrix(sapply(y, as.numeric)))
```

```{r}

xgb_train_rmse <- numeric(nrow(grid))
xgb_test_rmse <- numeric(nrow(grid))


for(i in 1:nrow(grid)){
  xgb_untuned = xgb.cv(
    data = encuesta_mrp_m,
    booster = "gblinear",
    lambda = grid$lambda[i],
    alpha = grid$alpha[i],
    lambda_bias = grid$lambda_bias[i],
    max_depth = grid$max_depth[i],
    objective = "reg:squarederror",
    eval_metric = "rmse",
    nrounds = 1000,
    early_stopping_rounds = 3, # training with a validation set will stop if the performance does not improve for k rounds (3)
    nfold = 5
  )
  
  xgb_train_rmse[i] <-
    xgb_untuned$evaluation_log$train_rmse_mean[xgb_untuned$best_iteration]
  xgb_test_rmse[i] <-
    xgb_untuned$evaluation_log$test_rmse_mean[xgb_untuned$best_iteration]
  
  cat(i, "\n")
}
```

```{r, eval=FALSE}
xgb_train_rmse <- xgb_train_rmse %>% 
  tibble() %>% 
  mutate(simulacion = seq(1:1715))

xgb_test_rmse <- xgb_test_rmse %>% 
  tibble() %>% 
  mutate(simulacion = seq(1:1715))
````

```{r}
#Guardamos las iteraciones para el primer modelo de la validación cruzada

saveRDS(xgb_train_rmse, file = "ingreso/output/xgb_train_rmse_all_redu.rds")
# saveRDS(xgb_test_rmse, file = "ingreso/output/xgb_test_rmse_all_redu.rds")

```


```{r}
xgb_train_rmse <- readRDS("ingreso/output/xgb_train_rmse_all_redu.rds")
```

```{r}
xgb_train_rmse %>% ggplot(aes(x = simulacion, y = .)) + 
  geom_line()
```

```{r}
grid[which.min(xgb_train_rmse$.), ]

xgb_train_rmse %>% ggplot(aes(x = simulacion, y = .)) + 
  geom_line() +
  geom_vline(xintercept = 34, linetype = 2, col = "red") +
  xlim(0, 100) +
  theme_minimal() 
```

vamos a ajustar el modelo con los siguientes hiperparámetros:

- max_depth = 6
- lambda = 0
- lambda_bias = 0.6
- alpha = 0.75

```{r, eval=FALSE}
modelo <- xgboost(
  booster = "gblinear",
  objective = "reg:squarederror",
  max_depth = 6,
  lambda = 0,
  lambda_bias = 0.6,
  alpha = 0.75,
  nrounds = 1000,
  early_stopping_rounds = 3,
  data = encuesta_mrp_m
)

ggplot(modelo$evaluation_log, aes(
  x = iter,
  y = train_rmse
)) + geom_line() 

# Valor minimo
modelo$evaluation_log[which.min(modelo$evaluation_log$train_rmse), ]
eval <- modelo$evaluation_log

ggplot(modelo$evaluation_log, aes(
  x = iter,
  y = train_rmse
)) + geom_line() +
  geom_vline(xintercept = 261, col = "red", linetype = 2) + theme_minimal()
```

```{r}
saveRDS(modelo, file = "ingreso/output/modelo_redu.rds")
```



